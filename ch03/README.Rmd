---
title: "Properties of regression"
author:
date:
always_allow_html: yes
output:
  github_document:
    toc: true
    toc_depth: 2
editor_options: 
  chunk_output_type: console
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r setup, echo = F, message = F}
knitr::opts_chunk$set(
  cache = T,
  cache.path = '../cache/',
  fig.path = '../fig/',
  message = F,
  warning = F
  )

library(hrbrthemes)
library(mixtape)
library(tidyverse)
library(magrittr)
library(janitor)
library(latex2exp)
library(skimr)
library(haven)
library(broom)
library(kableExtra)
library(broom)
```

## OLS regression line

```{r ols_line}

set.seed(1)

# construct the data
dat <- tibble(
  x = rnorm(1E4, 0, 1),
  u = rnorm(1E4, 0, 1),
  y = 5.5 * x + 12 * u
  )

# regress y on x 
reg <- lm(y ~ x, data = dat)

# create named vector of coefficient estimates
coefs <- tidy(reg) %>% pull(estimate) 
names(coefs) <- tidy(reg) %>% pull(term)

round(coefs, 3)

# fitted values and residuals (two ways to recover them)
preds_resids <- reg %>% 
  augment() %>% 
  mutate(yhat1 = .fitted,
         yhat2 = coefs["(Intercept)"] + coefs["x"] * dat$x,
         uhat1 = .resid,
         uhat2 = y - yhat2)

# check equality
preds_resids %>% 
  summarise(all.equal(yhat1, yhat2) & all.equal(uhat1, uhat2))
```



```{r ols1a}
# figure 3
ggplot(preds_resids, aes(x, y)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = 'lm', col = ipsum_pal()(1)) +
  labs(title = 'OLS Regression Line') +
  theme_ipsum()
```

```{r ols1b}
# figure 4
ggplot(preds_resids, aes(yhat1, uhat1)) +
  geom_point(alpha = 0.5) +
  labs(x = 'Fitted Values', y = 'Residuals') +
  theme_ipsum()
```

## Algebraic properties of OLS

### Show that the sum of the OLS residual always adds up to zero

```{r ols_prop, results='asis'}

set.seed(1234)

# construct the data
dat <- tibble(
  x = 9 * rnorm(10, 0, 1),
  u = 36 * rnorm(10, 0, 1),
  y = 3 + 2 * x + u
  )

reg <- lm(y ~ x, data = dat)

# fitted values and residuals (two ways to recover them)
reg %>% 
  augment() %>% 
  mutate(id = row_number(),
         yhat = .fitted,
         uhat = .resid,
         x_uhat = x * uhat, 
         yhat_uhat = yhat * uhat) %>% 
  select(id, x, uhat, y, yhat, uhat, x_uhat, yhat_uhat) %>% 
  adorn_totals() %>% 
  adorn_rounding(digits = 3) %>% 
  # styling table
  knitr::kable() %>% 
  kable_styling() %>% 
  row_spec(11, bold = TRUE)
```

```{r}
means <- dat %>% summarise_all(mean)

tidy(reg) %>% 
  select(term, estimate) %>% 
  spread(term, estimate) %>% 
  mutate(av_pred = `(Intercept)` + means$x * x, 
         mean_y = means$y)
```

## Unbiasness: Monte Carlo simulation of OLS

```{r ols_value, results='asis'}

# ols function
ols <- function(...) {
  dat <- tibble(
    x = 9 * rnorm(1E4, 0, 1),
    u = 36 * rnorm(1E4, 0, 1),
    y = 3 + 2 * x + u
    ) %>% 
    lm(y ~ x, data = .)
}

betas_df <- map_df(1:1E3, ~ tidy(ols(.)), .id = "id") %>% 
  unnest() %>% 
  filter(term == "x") %>% 
  select(beta = estimate) 

skim(betas_df) %>% skimr::kable()

# figure 5
ggplot(betas_df, aes(x = beta, y = ..density..)) +
  geom_histogram() +
  geom_vline(xintercept = 2, col = "red") + 
  labs(title = "Distribution of coefficients from Monte Carlo simulation", 
       x = TeX("$\\beta_{x}$", ), y = 'Density') +
  theme_ipsum()
```


## Regression anatomy theorem

```{r ols_anatomy}
# auto dataset
auto <- read_dta('http://www.stata-press.com/data/r8/auto.dta') %>% 
  # cleaning up some of the Stata metadata
  zap_formats() %>% 
  zap_labels() 

# add the residuals to the data frame 
auto %<>% 
  mutate(length_resid = residuals(lm(length ~ weight + headroom + mpg, 
                                     data = .)))

# create a data frame of estimates from each regression in the list
coefs <- list(bivariate = price ~ length,
              multivariate = price ~ length + weight + headroom + mpg,
              aux1 = length ~ weight + headroom + mpg,
              aux2 = price ~ length_resid) %>% 
  map_df(.f = ~ tidy(lm(.x, data = auto)), 
         .id = "reg") 

# select the coefficients from the original regression and 
# the partialled-out version
coefs %>% 
  filter(term %in% c("length", "length_resid")) %>% 
  select(reg:estimate) %>% 
  knitr::kable() 
```

OLS slope estimate: 
$$\hat \beta_1 = \frac{C(x,y)}{Var(x)} $$

```{r}
# OLS estimate 
auto %>% 
  summarise(beta = cov(price, length_resid) / var(length_resid))

pauto <- bind_rows(list(BV = auto, MV = auto), .id = "type") %>% 
  mutate(length = case_when(
    type == "BV" ~ length - mean(length), 
    TRUE ~ length_resid)) %>% 
  select(price, length, type)

# shift factor (mean adjustment of length requires adjustment of intercept)
s_factor <- coefs %>% 
  filter(reg == "bivariate", term == "length") %>% 
  pull(estimate) * mean(auto$length)
```

```{r reganatomy}
coefs_filt <- coefs %>% 
  filter(reg %in% c("bivariate", "aux2")) %>% 
  mutate(estimate = case_when(
    term == "(Intercept)" & reg == "bivariate" ~ estimate + s_factor,
    TRUE ~ estimate),
         term = case_when(
           term == "length_resid" ~ "length",
           TRUE ~ term)) %>% 
  select(reg:estimate) %>% 
  spread(term, estimate) 

ggplot(pauto) + 
  geom_point(aes(length, price, colour = type)) + 
  scale_colour_ipsum(name = 'Type')  + 
  geom_abline(data = coefs_filt, 
              aes(intercept = `(Intercept)`, 
                  slope = length, 
                  col = reg)) + 
  labs(title = 'Regression Anatomy', 
       x = 'Length', y = 'Price') +
  theme_ipsum() 
```
